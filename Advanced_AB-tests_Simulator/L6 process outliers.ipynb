{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Задача 3. Реализация метода удаления выбросов\n",
    "  \n",
    "\n",
    "Реализуйте функцию для обработки выбросов в значениях метрики."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте метод process_outliers класса MetricsService."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Design(BaseModel):\n",
    "    \"\"\"Дата-класс с описание параметров эксперимента.\n",
    "\n",
    "    statistical_test - тип статтеста. ['ttest', 'bootstrap']\n",
    "    effect - размер эффекта в процентах\n",
    "    alpha - уровень значимости\n",
    "    beta - допустимая вероятность ошибки II рода\n",
    "    bootstrap_iter - количество итераций бутстрепа\n",
    "    bootstrap_ci_type - способ построения доверительного интервала. ['normal', 'percentile', 'pivotal']\n",
    "    bootstrap_agg_func - метрика эксперимента. ['mean', 'quantile 95']\n",
    "    metric_name - название целевой метрики эксперимента\n",
    "    metric_outlier_lower_bound - нижняя допустимая граница метрики, всё что ниже считаем выбросами\n",
    "    metric_outlier_upper_bound - верхняя допустимая граница метрики, всё что выше считаем выбросами\n",
    "    metric_outlier_process_type - способ обработки выбросов. ['drop', 'clip'].\n",
    "        'drop' - удаляем измерение, 'clip' - заменяем выброс на значение ближайшей границы (lower_bound, upper_bound).\n",
    "    \"\"\"\n",
    "    statistical_test: str = 'ttest'\n",
    "    effect: float = 3.\n",
    "    alpha: float = 0.05\n",
    "    beta: float = 0.1\n",
    "    bootstrap_iter: int = 1000\n",
    "    bootstrap_ci_type: str = 'normal'\n",
    "    bootstrap_agg_func: str = 'mean'\n",
    "    metric_name: str\n",
    "    metric_outlier_lower_bound: float\n",
    "    metric_outlier_upper_bound: float\n",
    "    metric_outlier_process_type: str\n",
    "\n",
    "\n",
    "class MetricsService:\n",
    "\n",
    "    def process_outliers(self, metrics, design):\n",
    "        \"\"\"Возвращает новый датафрейм с обработанными выбросами в измерениях метрики.\n",
    "\n",
    "        :param metrics (pd.DataFrame): таблица со значениями метрики, columns=['user_id', 'metric'].\n",
    "        :param design (Design): объект с данными, описывающий параметры эксперимента.\n",
    "        :return df: columns=['user_id', 'metric']\n",
    "        \"\"\"\n",
    "        # YOUR_CODE_HERE\n",
    "        if design.metric_outlier_process_type == 'drop':\n",
    "            # Удаляем все значения, которые выходят за границы.\n",
    "            metrics = metrics[(metrics['metric'] >= design.metric_outlier_lower_bound) & \n",
    "                              (metrics['metric'] <= design.metric_outlier_upper_bound)]\n",
    "        elif design.metric_outlier_process_type == 'clip':\n",
    "            # Заменяем все значения, которые выходят за границы, на ближайшую границу.\n",
    "            metrics.loc[metrics['metric'] < design.metric_outlier_lower_bound, 'metric'] = design.metric_outlier_lower_bound\n",
    "            metrics.loc[metrics['metric'] > design.metric_outlier_upper_bound, 'metric'] = design.metric_outlier_upper_bound\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid outlier processing type: {design.metric_outlier_process_type}\")\n",
    "\n",
    "        return metrics\n",
    "\n",
    "\n",
    "def _chech_df(df, df_ideal, sort_by, reindex=False, set_dtypes=False):\n",
    "    assert isinstance(df, pd.DataFrame), 'Функция вернула не pd.DataFrame.'\n",
    "    assert len(df) == len(df_ideal), 'Неверное количество строк.'\n",
    "    assert len(df.T) == len(df_ideal.T), 'Неверное количество столбцов.'\n",
    "    columns = df_ideal.columns\n",
    "    assert df.columns.isin(columns).sum() == len(df.columns), 'Неверное название столбцов.'\n",
    "    df = df[columns].sort_values(sort_by)\n",
    "    df_ideal = df_ideal.sort_values(sort_by)\n",
    "    if reindex:\n",
    "        df_ideal.index = range(len(df_ideal))\n",
    "        df.index = range(len(df))\n",
    "    if set_dtypes:\n",
    "        for column, dtype in df_ideal.dtypes.to_dict().items():\n",
    "            df[column] = df[column].astype(dtype)\n",
    "    assert df_ideal.equals(df), 'Итоговый датафрейм не совпадает с верным результатом.'\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    metrics = pd.DataFrame({\n",
    "        'user_id': ['1', '2', '3'],\n",
    "        'metric': [1., 2, 3]\n",
    "    })\n",
    "    design = Design(\n",
    "        metric_name='response_time',\n",
    "        metric_outlier_lower_bound=0.1,\n",
    "        metric_outlier_upper_bound=2.2,\n",
    "        metric_outlier_process_type='drop',\n",
    "    )\n",
    "    ideal_processed_metrics = pd.DataFrame({\n",
    "        'user_id': ['1', '2'],\n",
    "        'metric': [1., 2]\n",
    "    })\n",
    "\n",
    "    metrics_service = MetricsService()\n",
    "    processed_metrics = metrics_service.process_outliers(metrics, design)\n",
    "    _chech_df(processed_metrics, ideal_processed_metrics, ['user_id', 'metric'], True, True)\n",
    "    print('simple test passed')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
